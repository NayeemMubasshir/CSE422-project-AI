# -*- coding: utf-8 -*-
"""Main 422.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-CyuQfkkZrjKLLNPAcvszkCWeb5h-5m-

# Importing Libraries and Data Visualization
"""

# Some library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

from sklearn.model_selection import train_test_split
from sklearn import datasets, preprocessing, linear_model
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Read CSV file from Google Drive
file_path = '/content/drive/MyDrive/results.csv'
res = pd.read_csv("https://drive.google.com/uc?id=1rC1J3LrIQ0LfeOqq8__hbWmIuxsfrudV")

# All columns for frequency histogram
cols = ['Age', 'Sex', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Stroke', 'HighBP']

res = pd.read_csv('/content/drive/MyDrive/CSE422/Project/results.csv') # read csv file
# All columns for frequency histogram
cols = ['Age', 'Sex', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Stroke', 'HighBP', 'Diabetes']

res.isnull().sum() # Checking null values (not found any)

for i in res.columns:
  print(f"{i} - {res[i].nunique()}")

res.describe()

res.info()

# Correlation.
# Greater positive means if x increases, y increases and vice-versa
# Greater negative means if x increases, y decreases and vice-versa
plt.figure(figsize=(16,10))
sb.heatmap(res.corr(),annot=True, fmt='.1g')

# Plotting frequency
for i in cols:
    plt.figure()
    plt.hist(res[i])
    plt.xlabel(i)
    plt.ylabel("Frequency")
    plt.title(f"{i}")
    plt.show()

"""# Without Data Preprocessing"""

x = res.drop(['Diabetes'], axis=1) # Excluding diabetes, as it is our target
x

y = res['Diabetes'] # Our target
y

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=40)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train, y_train)
print("Training accuracy (%) =", lr.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", lr.score(x_test,y_test)*100)

model = linear_model.LogisticRegression(C=100, fit_intercept=True, solver='lbfgs', max_iter=500)
model = model.fit(x_train, y_train)
y_predict = model.predict(x_test)

acc_training = np.mean(y_train == model.predict(x_train))*100
acc_testing = np.mean(y_test == y_predict)*100
print("Training accuracy (%) =", acc_training)
print("Testing accuracy (%) =", acc_testing)

from sklearn.tree import DecisionTreeRegressor
tree=DecisionTreeRegressor()
tree.fit(x_train,y_train)

print("Training accuracy (%) =", tree.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", tree.score(x_test,y_test)*100)

from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier()
tree.fit(x_train,y_train)

print("Training accuracy (%) =", tree.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", tree.score(x_test,y_test)*100)

from sklearn.ensemble import RandomForestRegressor
forest=RandomForestRegressor()
forest.fit(x_train,y_train)

print("Training accuracy (%) =", forest.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", forest.score(x_test,y_test)*100)

from sklearn.ensemble import RandomForestClassifier
forest=RandomForestClassifier()
forest.fit(x_train,y_train)

print("Training accuracy (%) =", forest.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", forest.score(x_test,y_test)*100)

from sklearn.neural_network import MLPClassifier # Multi-layer perceptron
model = MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=(20,5), alpha=0.001, activation="relu") # hl = (number of layer, number of neuron) alpha = regularization term
# how to tune these hyperparameters - activation, hidden layer size


model = model.fit(x_train, y_train)
y_predict = model.predict(x_test)

acc_training = np.mean(y_train == model.predict(x_train))*100
acc_testing = np.mean(y_test == y_predict)*100
print("Training accuracy (%) =", acc_training)
print("Testing accuracy (%) =", acc_testing)

import matplotlib.pyplot as plt

# Create a list of tuples containing the model name and its accuracy scores
models = [('Linear Regression', lr.score(x_train,y_train)*100, lr.score(x_test,y_test)*100),
          ('Logistic Regression', acc_training, acc_testing),
          ('Decision Tree', tree.score(x_train,y_train)*100, tree.score(x_test,y_test)*100),
          ('Random Forest', forest.score(x_train,y_train)*100, forest.score(x_test,y_test)*100),
          ('MLP', acc_training, acc_testing)]

# Create lists for model names, training accuracies, and testing accuracies
model_names = [model[0] for model in models]
training_accs = [model[1] for model in models]
testing_accs = [model[2] for model in models]

# Plot the accuracies
plt.figure(figsize=(10, 6))
plt.bar(model_names, training_accs, width=0.4, label='Training Accuracy')
plt.bar([name + ' (test)' for name in model_names], testing_accs, width=0.4, label='Testing Accuracy')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()

"""# With Data Preprocessing"""

res = pd.read_csv('/content/drive/MyDrive/CSE422/Project/results2.csv') # read csv file, with null values
# All columns for frequency histogram
cols = ['Age', 'Sex', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Stroke', 'HighBP', 'Diabetes']

res.isnull().sum() # Checking null values (found)

impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(res[['Age']])
res['Age'] = impute.transform(res[['Age']])

impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(res[['BMI']])
res['BMI'] = impute.transform(res[['BMI']])

res.isnull().sum() # Checking null values (not found any)

from pandas.plotting import scatter_matrix
scatter_matrix(res[["Age", "BMI", "Diabetes"]], c=res["Diabetes"] , figsize=(12,12), alpha=1)

res = res.drop([cols[1], cols[5], cols[7], cols[8], cols[9], cols[10], cols[12]], axis=1)
res

x = res.drop(['Diabetes'], axis=1)
y = res['Diabetes']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=40)

scaler=StandardScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train, y_train)
print("Training accuracy (%) =", lr.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", lr.score(x_test,y_test)*100)

model = linear_model.LogisticRegression(C=100, fit_intercept=True, solver='lbfgs', max_iter=500)
model = model.fit(x_train, y_train)
y_predict = model.predict(x_test)

acc_training = np.mean(y_train == model.predict(x_train))*100
acc_testing = np.mean(y_test == y_predict)*100
print("Training accuracy (%) =", acc_training)
print("Testing accuracy (%) =", acc_testing)

from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier()
tree.fit(x_train,y_train)

print("Training accuracy (%) =", tree.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", tree.score(x_test,y_test)*100)

from sklearn.ensemble import RandomForestClassifier
forest=RandomForestClassifier()
forest.fit(x_train,y_train)

print("Training accuracy (%) =", forest.score(x_train,y_train) * 100)
print("Testing accuracy (%) =", forest.score(x_test,y_test)*100)

from sklearn.neural_network import MLPClassifier # Multi-layer perceptron
model = MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=(20,5), alpha=0.001, activation="relu") # hl = (number of layer, number of neuron) alpha = regularization term
# how to tune these hyperparameters - activation, hidden layer size


model = model.fit(x_train, y_train)
y_predict = model.predict(x_test)

acc_training = np.mean(y_train == model.predict(x_train))*100
acc_testing = np.mean(y_test == y_predict)*100
print("Training accuracy (%) =", acc_training)
print("Testing accuracy (%) =", acc_testing)

import matplotlib.pyplot as plt

# Create a list of tuples containing the model name and its accuracy scores
models = [('Linear Regression', lr.score(x_train,y_train)*100, lr.score(x_test,y_test)*100),
          ('Logistic Regression', acc_training, acc_testing),
          ('Decision Tree', tree.score(x_train,y_train)*100, tree.score(x_test,y_test)*100),
          ('Random Forest', forest.score(x_train,y_train)*100, forest.score(x_test,y_test)*100),
          ('MLP', acc_training, acc_testing)]

# Create lists for model names, training accuracies, and testing accuracies
model_names = [model[0] for model in models]
training_accs = [model[1] for model in models]
testing_accs = [model[2] for model in models]

# Plot the accuracies
plt.figure(figsize=(10, 6))
plt.bar(model_names, training_accs, width=0.4, label='Training Accuracy')
plt.bar([name + ' (test)' for name in model_names], testing_accs, width=0.4, label='Testing Accuracy')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()